### EDA : Exploratory Data Analysis (탐색적 데이터 분석)

- EDA란? 
  - 데이터를 탐색하고 이해하기 위한 과정
  - 데이터셋의 구성, 변수 간의 관계, 이상치 및 결측치의 여부 등을 파악
  - 데이터의 특성을 시각화 및 요약하여 데이터의 패턴과 통계적 성질을 탐색

- 표준 정규분포 : 평균이 0이고 표준편차가 1인 정규분포

- 평균은 이상치에 영향을 많이 받음 / 중위값은 이상치에 덜 민감

- 절사 평균 : 데이터에서 일정 비율만큼 가장 큰 / 가장 작은 부분은 제거 후 평균

- IQR : Inter Quatile Range 사분위 범위 

  - 상위 25%와 하위 25% 사이의 범위 (Q3 - Q1)
  - 데이터의 이상치를 탐지하는 데 사용됨

- nobs : 갯수

  minmax : (최소값, 최대값)

  mean : 평균

  variance : 분산

  skewness : 왜도(좌측으로 치우쳐져 있는 경우 음수 값, 우측으로 치우쳐져 있는 경우 양수 값)

  kurtosis : 첨도(확률 분포나 데이터의 꼬리의 두께나 중앙 부분의 뾰족함을 측정)

- 범위(Range) 구하는 방법 : ptp (peak to peak)

- 도수분포표 : 데이터의 값들이 특정 구간 또는 범주에 속하는 빈도(도수)를 보여주는 표

- 결측치 : 데이터셋에서 값이 비어있거나 없는 경우

 



### Machine Learning 이란?

- 문제를 풀기 위해서 컴퓨터 시스템에 데이터를 학습시켜서 모델을 만드는 것
- 지도학습 : 입력 데이터와 이에 해당하는 정답 데이터을 사용하여 모델을 학습
- 비지도학습 : 데이터가 주어지지 않은 상황에서 패턴이나 구조를 발견하고 모델을 학습하는 방법

- x : data,  독립변수, feature, 문제 
- y : target, 종속변수, label, 답
- 학습 : 입력받은 x와 y를 가지고 식을 만드는 것
- 옵티마이저(optimizer) : 가중치(w)와 편향(b)을 조정하여 손실 최소화 / 성능 지표 최대화하는 요소

- loss / cost function : 예측 값과 실제 값 사이의 오차를 계산하는 함수 (성능을 평가하고 최적화 하기 위해)
- 과대적합(overfitting) 
  - 주어진 입력 데이터에 비하여 모델의 복잡도가 너무 높아 
  - 입력 데이터의 잡음까지 fitting 하는 경향을 보이고, 일반화에 실패하는 상황 
  - 학습 데이터에만 과도하게 적합되어 실제 데이터에 대한 일반화 능력이 저하되는 현상
- 과소적합 (underfitting)
  - 주어진 입력 데이터에 비하여 모델의 복잡도가 너무 낮아 
  - 입력 데이터로부터 충분히 학습하지 못하는 상황



- 예측
  - 모델이 학습된 후에 입력 데이터에 대해 결과를 추론하는 과정
  - 모델은 학습 데이터로부터 학습한 내용을 활용하여 새로운 데이터에 대한 예측 값을 생성



- 분류
  - 주어진 입력 데이터를 사전에 정의된 클래스 레이블 중 하나에 할당하는 작업
  - 주로 범주형 데이터를 다루는데 사용



- train (학습)
  - 모델이 주어진 데이터에서 패턴과 특징을 학습하는 과정
  - 학습 데이터의 입력과 정답 레이블 사이의 관계를 파악하고, 최적의 매개변수를 찾아내는 방법을 학습



- test (모델 평가)
  - 학습된 모델의 성능을 평가하기 위해 테스트 데이터를 사용하는 과정
  - 모델은 학습 데이터에서 학습한 내용을 바탕으로 테스트 데이터의 입력에 대한 예측을 수행하고
  - 예측 결과를 정답 레이블과 비교하여 성능을 평가



### ERROR

- 입력된 값 (문제(x값) / 정답(y값))
- 궁극적인 목표! x 값을 주면 y 값을 도출하게 만들고 싶다 (x와 y의 관계)

- 정답과 예측의 차이를 단순하게 더하면 판단을 제대로 내릴 수 없음

- 오차들을 제곱해서 더한 뒤 평균을 내는 방법 : Mean Squared Error (평균 제곱 오차)
- MSE는 값이 작을수록 모델의 예측이 실제값과 가깝다





### Gradient Descent 경사 하강법

- y = w * x + b
- 평균 제곱 오차(MSE) : 예측값과 실제값의 차이를 제곱한 후 평균
- 기울기가 점점 줄어들게 만들고 싶다 > MSE를 줄이겠다 > 정답에 가까워지고 싶다

- w랑 b를 정답에 가까워지게 하겠다
- learning_rate
  - 머신러닝 알고리즘에서 최적화 과정에서 사용되는 하이퍼파라미터
  - 주로 경사 하강법과 같은 최적화 알고리즘에서 가중치와 편향을 조정하는 단계에서 사용
  - 손실 함수(Loss Function)를 최소화하는 가중치와 편향을 찾는 최적화 알고리즘 
  - 학습률은 가중치와 편향을 얼마만큼 업데이트할지를 결정하는 역할





### Linear Regression (선형 회귀)

- 회귀분석이란? 
  
- 종속 변수와 한 개 이상의 독립 변수간의 관계를 모델링하고 예측하기 위해 사용되는 분석 방법
  
- 선형 회귀 : 연속적인 데이터를 **예측**하고 싶을 때
- y : 출력 변수 / x : 입력 변수
- w : 가중치(회귀 계수) / b : 절편
- 데이터 분할 : 학습에 필요한 데이터와 테스트에 필요한 데이터가 다르다
- test_size=0.3 : 전체 데이터 중에서 30%만 테스트용으로 선택 (나머지 70%는 학습용)

- random_state : 랜덤으로 가져오는 시드 고정

- **다항 회귀 (PolynomialFeatures)**

  - 선형 회귀(Linear Regression)의 확장된 형태

  - 비선형 데이터를 모델링하는데 사용되는 회귀 기법

  - 독립 변수와 종속 변수 간의 비선형 관계를 다항식으로 모델링 함

  - 너무 낮은 차수는 데이터를 충분히 모델링하지 못하게 하고

  - 너무 높은 차수는 과적합(overfitting) 문제를 발생 시킴

    

- **편향(Bias)** 

  - 주어진 데이터에 대해 얼마나 정확한 예측을 할 수 있는지에 대한 척도
  - 편향이 높은 모델은 주어진 데이터의 다양한 패턴과 특징을 캡처하지 못하고, 
  - 너무 간단하거나 한정적인 형태로 데이터를 설명하려고 시도하기 때문에 
  - 예측 능력이 제한됨 (과소 적합)

- **분산(Variance)**

  - 주어진 데이터에 대해 얼마나 민감한지를 나타냄
  - 분산이 높으면 모델이 작은 변동성에도 민감하게 반응하여 주어진 데이터에 대해 높은 정확도
  - 하지만 다른 데이터에 대해서는 잘 일반화하지 못함 (과적합)

- **편향과 분산의 트레이드 오프 관계**

  - 편향과 분산은 서로 반비례하는 관계. 

  - 모델의 복잡성을 증가시키면 분산은 증가하고 편향은 감소

  - 너무 단순한 모델은 편향이 높아 정확성이 떨어짐

  - 너무 복잡한 모델은 분산이 높아 일반화 성능이 저하됨

  - 최적의 모델을 찾기 위해서는 데이터의 복잡성과 특성, 모델의 파라미터 등을 고려하여 

  - 적절한 편향-분산 트레이드 오프를 찾는 것이 중요함

    

- **선형회귀가 과적합 되는 것을 방지하려면?**

- 가중치에 제약조건을 추가해서 정규화 선형 회귀 기법을 써야 함!

  1) 릿지 (Ridge)

  - L2 규제를 적용하여 모델의 가중치를 조정 (제곱의 합에 패널티 부여)
  - L2 규제는 가중치의 크기를 작게 유지하고 일반화 성능을 향상

  2) 로쏘 (Lasso)

  - L1 규제를 적용하여 모델의 가중치를 조정
  - L1 규제 항은 가중치들의 절댓값의 합에 페널티를 부여하는 것

  3) 엘라스틱넷 (ElasticNet)

  -  L1 규제와 L2 규제를 함께 적용하는 정규화 기법





### 로지스틱 회귀 분석 (Logistic Regression Analysis)

- 연속적이지 않고 이산적인 데이터를 분류
- 데이터를 가장 잘 분리할 수 있는 선을 그리자

- 분류(Classification) 작업에 사용되는 지도학습 알고리즘
- 입력 값을 0과 1 사이의 확률 값으로 변환
- 입력 변수와 출력 변수(이진 레이블) 사이의 관계를 모델링





### metrix(평가지표)

- MAE (Mean Absolute Error) : 예측 값과 실제 값 간의 평균적인 절대적인 오차를 나타내는 지표
- MSE (Mean Squared Error) : 예측 값과 실제 값 간의 평균적인 제곱 오차를 나타내는 지표
- RMSE (Root Mean Squared Error) : 예측 값과 실제 값 간의 평균 제곱근 오차를 나타내는 지표

- R Squared (결정계수) 
  - 실제 종속 변수 값과 예측 값 간의 제곱 오차의 분산을 (SSR)
  - 종속 변수 값의 분산으로 나눈 비율 (SST)



- 혼동행렬 (confusion metrix)
  - 모델의 예측 결과를 실제 값과 비교하여 모델의 성능을 평가하는 데 사용되는 행렬
  - 예측된 클래스와 실제 클래스 간의 관계를 표현하며
  - 주로 이진 분류(Binary Classification)에서 사용
  - TP (True Positive) / FP (False Positive) / TN (True Negative) / FN (False Negative)



- 정확도(Accuracy) : (TP+FP+FN+TN) / (TP+TN) (다 더한 걸 T로만 나눔)

  - 전체 관측치 중에서 실제값과 예측치가 일치하는 정도

    

- 정밀도(Precision) : (TP) / (TP+FP)

- 재현율(Recall 또는 Sensitivity) : (TP+FN) / (TP)
- f1_score : Presision과 Recall의 조화 평균
  - 2 * (precision * recall / (precision + recall))
  - 2를 곱하여 범위를 조정하면 0부터 1까지의 값을 가지게 하기 위해서



- TPR (True Positive Rate)
  - TP / (TP + FN) (=Recall) 
  - 실제 Positive인 샘플 중에서 모델이 Positive로 정확하게 예측한 비율
- FPR (False Positive Rate)
  - FP / (FP + TN) 
  - 실제 Negative인 경우를 잘못하여 Positive로 잘못 예측하는 비율



- ROC (Receiver Operating Characteristic)
  - 모델의 임계값(Threshold)을 변화시키면서, 
  - 분류 모델의 민감도(Sensitivity)와 특이도(Specificity) 사이의 관계를 시각화
  - x축에 (1 - 특이도) / y축에 민감도



- AUC (Area Under the ROC Curve)
  - ROC 곡선의 아래 영역을 나타내는 지표
  - 0과 1 사이의 값을 가지며, 1에 가까울수록 모델의 성능이 좋다고 해석
  - AUC가 0.5에 가까울 경우, 모델의 성능은 무작위 예측에 가까우며, 
  - 0.5보다 낮을 경우 모델의 예측 성능이 뒤집혀 있음



- 스레드 홀드 (threshold)
  - 이진 분류(Binary Classification) 모델에서 분류 결정을 내리는 기준값



- 분류 결정 임곗값이 낮아질수록 positive로 예측할 확률이 높아짐, 재현율 증가 (O)
- 정밀도는 감소 (정밀도와 재현율 trade-off관계)



### 스케일링 (Scaling)

- 데이터의 범위를 조정하여 모든 특성이 동일한 스케일을 갖도록 하는 전처리 과정
- 다양한 머신 러닝 알고리즘에서 중요한 단계로 고려되며, 
- 데이터의 크기나 범위가 다른 경우에도 모델의 성능을 향상

- MinMaxScaler 
  - 데이터의 최솟값과 최댓값을 이용하여 원래 데이터를 **0과 1 사이의 범위로 변환**

- StandardScaler 
  - 정규분포를 따른다는 가정 하에 정규분포 형식으로 바꿈
  - 데이터의 평균과 표준편차를 이용
  - 데이터를 평균이 0이고 표준편차가 1인 분포로 변환하여 데이터의 스케일을 조정

- Normalizer 
  - 0 ~ 1 (유클리드 거리 치환)
  - 각 데이터 포인트를 벡터의 길이로 나누어 데이터를 단위 길이로 조정





### SVM (Support Vector Machine)

- 분류 문제에 많이 활용되며, 특히 선형적으로 구분되지 않는 데이터를 분류하는데 강점이 있음
- 데이터 포인트들을 고차원의 특성 공간으로 매핑하여 결정 경계를 찾음
- 클래스 간의 간격(마진)을 최대화하는 결정 경계를 찾는 것이 목표





### k겹 교차 검증 (k fold cross validation)

- 검증 (Validation) 
  - 모델의 성능을 평가하고 하이퍼파라미터 튜닝 등 모델의 설정을 조정하기 위해 사용되는 데이터 세트
  - 모의고사를 보는 것처럼 중간중간 체크하려고 (나중에 학습할 때 넣어도 가능함)

- 주어진 데이터를 K개의 폴드(fold)로 나누고, 각 폴드를 한 번씩 검증 세트로 사용
- 나머지 K-1개 폴드를 훈련 세트로 사용하여 모델을 학습하고 성능을 평가하는 과정을 K번 반복

- k-fold 교차 검증의 단계
  - 데이터셋을 k개의 동일한 크기로 분할(이때 데이터는 무작위로 섞어서)
  - 모델을 k-1개의 폴드로 학습시키고
  - 남은 1개의 폴드는 검증에 사용
  - 학습된 모델을 검증 폴드로 평가하여 성능 점수를 계산
  - 2번과 3번의 과정을 k번 반복하여 모든 폴드에서 성능 점수를 계산
  - 최종적으로 얻은 k개의 성능 점수의 평균 또는 통계량을 계산
  - 모델의 성능을 평가

- cross_val_score : 교차 검증(Cross Validation)을 수행하여 모델의 성능을 평가하는 함수





### 하이퍼 파라미터 튜닝 (Hyperparameter tuning)
- 머신 러닝 모델을 최적화하기 위해 하이퍼 파라미터들을 최적의 조합으로 찾는 과정
- 모델의 학습 결과를 좀 더 올리기 위해서 스코어를 올리기 위해 파라미터를 바꿔보는 것
- test 셋은 모델이 잘 만들어져 있는지 마지막에 딱 한 번만 씀
- vaild 셋은 학습할 때 잘 되어있는지 반복적으로 씀

- C(cost) : 이상치 허용 (낮으면 많이 허용, 높으면 적게 허용)
- gamma : 경계의 복잡도 (서포트 벡터의 영향력이 미치는 거리 - 표준 편차)
- gamma=gamma, C=C : 모델의 파라미터 부분을 바꾸는 것이 영향을 많이 미침 
- 이걸 찾는 과정이 하이퍼 파라미터 튜닝
- **그리드 서치(Grid Search)**
  - 가능한 모든 조합을 대상으로 하이퍼 파라미터 공간을 탐색하여 최적의 조합을 찾는 방법
  - 조합들을 모두 탐색하여 모델을 학습하고 검증 데이터를 사용하여 성능을 평가
  - 가장 우수한 조합과 해당 조합에서의 성능을 반환





### 나이브 베이즈(Naive Bayes)

- 사전 확률(prior)을 기반으로 사후확률(posterior)를 계산(추론)
- 확률 기반의 지도학습 알고리즘 중 하나로 각 특징(변수)이 서로 독립적이라고 가정
- 간단하고 빠른 속도로 학습과 예측이 가능하며, 작은 크기의 훈련 데이터에도 잘 동작
- GaussianNB : 연속적인 특성 (정규분포)
- BernoulliNB : 이진 특성
- MultinomialNB : 카운트 특성 (ex. 단어의 갯수)





### KNN (K-Nearest Neighbors)

- 주어진 데이터 포인트 주변에 있는 k개의 가장 가까운 이웃 데이터 포인트들을 기반으로 예측을 수행
- 간단하면서도 직관적인 알고리즘
- 새로운 데이터 포인트를 분류하거나 예측하기 위해 주변 이웃들의 레이블 또는 값에 기반하여 결정





### Decision Tree (의사 결정 나무)

- 스무 고개 놀이처럼 동작
- 조건에 대한 참과 거짓을 물어보면서 맞혀 나가는 과정
- 기준을 생성하고 True / False 과정을 거쳐 뻗어 나감
- 학습을 통해 트리 형태의 기준을 생성
- 지도학습 알고리즘 중 하나로, 데이터의 속성과 레이블 간의 관계를 나타내는 분류 기법
- 나무 구조를 가지며, 각 내부 노드는 속성을 테스트하고, 각 가지는 속성의 가능한 값에 대한 조건을 나타냄

- 불순도 (impurity)
  - 의사결정나무 알고리즘에서 분할 기준을 결정하는 데 사용되는 개념
  - 정보 균일도가 높은 데이터 셋을 먼저 선택할 수 있도록 규칙 조건을 만드는 것이 중요
  - 불순도를 계산하는 데에 사용되는 지표로 Gini 불순도(Gini Impurity)와 엔트로피(Entropy)를 주로 사용

- 최대 깊이 (max_depth)
  - 의사결정트리에서 최대 허용 깊이를 지정하는 매개변수
  - 트리의 깊이는 모델의 복잡성을 제어하며, 과적합을 방지하기 위해 적절한 값으로 설정





### 앙상블(ensemble)

- 여러 개의 모델(classifier)을 사용해서 예측 결합함으로써 보다 정확한 최종 예측을 도출하는 방법

- 단일 모델보다 더 다양한 관점에서 데이터를 학습하고 예측하기 때문에 일반적으로 성능이 우수함

- 결과가 안 좋은 모델들(약한 분류기)을 모아서 좋은 결과를 내도록 하는 것
- 각 모델은 독립적으로 학습하고 예측한 후, 그 결과를 조합하여 최종 예측



- **앙상블의 종류**

  - Stacking : 여러개의 개별적 모델을 학습하여 예측한 데이터셋을 최종 메타 모델이 사용하여 결과 도출

    

  - voting :  서로 다른 여러 모델의 예측을 조합하여 최종 예측을 수행

    - hard voting (여러 모델 중 다수결로 더 많은 것 선택) 

    - soft voting(가중치를 부여한 평균)

      

  - Bagging : 같은 유형의 모델을 병렬 결합 (Bootstrapping aggregation) / 중복 허용

    - 같은 모델들이 데이터를 서로 다르게 학습

    - 배깅 안에 랜덤 포레스트(Random Forest)가 있음

    - Random Forest : dicision tree가 여러개 (bagging)

    - 부트스트래핑 (Bootstrapping) : 원본 데이터셋에서 중복을 허용하여 샘플을 추출

      

  - Boosting : 여러 개의 모델들을 순차적으로 학습하여 결과에 대한 가중치를 부여

    - 약한 학습기(Weak Learner)를 여러 번 연속적으로 학습
    - 이전모델의 학습 결과를 받아서 다시 사용
    - 이전 모델이 잘못 예측한 샘플에 가중치를 부여하여 학습
    - 오답에 높은 가중치 > 오답을 정답으로 맞히기 위해
    - Gradient Boosting : 이전 모델의 오차를 보완하는 방식으로 모델을 순차적으로 학습
      
      - 그래디언트(기울기)를 활용하여 모델을 보완
      
        
      
        

  - XGBoost (eXtreme Gradient Boosting)
    - 부스팅(Boosting) 알고리즘을 기반으로 한 머신러닝 알고리즘
    - 특히 분류(Classification)와 회귀(Regression) 문제를 해결하는데 널리 사용
    - 다양한 분야에서 좋은 성능을 보임
    - 대회나 실제 문제에서 많이 사용되는 머신러닝 알고리즘 중 하나
    - 일반 gbm보다 빠름
    - 자체 과적합 규체 기능이 탑재됨

    

  - LightGBM (Light Gradient Boosting Machine)

    - Microsoft에서 개발한 고성능 분산 그래디언트 부스팅 프레임워크
    - xgboost와 매우 비슷 (속도가 더 빠르고 가볍다)
    - XGB 등 이전의 decision tree 기반 boosting은 높은 정밀도,  분할하는데 시간 걸림





### 군집 분석 (Clustering)

- 유사한 특성을 가진 데이터들을 그룹으로 묶는 비지도 학습 기법
- 유사한 데이터끼리 같은 그룹에 속하고, 서로 다른 그룹 간에는 상이한 특성을 가지도록 데이터를 분할





### 차원 축소(Dimensionality Reduction) 

- 피처 엔지니어링(Feature Engineering)은 머신러닝과 데이터 분석에서 중요한 작업 중 하나 
- 데이터의 특성을 유지하면서 차원(특성의 개수)을 감소시키는 기법을 의미
- 주성분 분석(PCA, Principal Component Analysis) 
- t-SNE(t-Distributed Stochastic Neighbor Embedding) 이 2가지가 대표적



- **주성분 분석(PCA, Principal Component Analysis)**
  - 다차원 데이터의 특성을 압축하여 데이터의 주요한 구조를 파악하는 기법
  - 주성분 분석은 고차원 데이터의 분산을 최대화하는 방향으로 데이터를 변환하여 새로운 특성 축을 생성
  - 가장 널리 사용되는 차원 축소 알고리즘
  - 원본 데이터의 분산을 최대한 보존하는 방향(주성분)을 찾아 차원을 줄임
  - 주성분은 데이터의 분산을 가장 크게 설명하는 방향으로 정렬



- **t-SNE(t-Distributed Stochastic Neighbor Embedding)**
  - 데이터를 저차원으로 매핑하는 비선형 차원 축소 기법
  - 고차원의 데이터를 저차원에 투영하여 시각화하는 데 많이 사용
  - t-SNE는 시각화를 위해 사용되는 기법이기 때문에 데이터의 군집 구조를 보존하는데 강점
  - 다른 차원 축소 기법과 달리 데이터 포인트들의 순서를 보존하지 않음
  - 새로운 데이터에 적용하는 것은 적합하지 않음
  - 시간과 계산 비용이 많이 소요될 수 있으므로 큰 데이터셋에 적용할 때 주의





### K-Means

- 비지도 학습의 대표적인 클러스터링 알고리즘
- 주어진 데이터를 K개의 클러스터로 그룹화하는 알고리즘으로, 각 클러스터의 중심을 기준으로 데이터 할당
- 각 클러스터의 중심과 데이터 간의 거리를 최소화하는 방식으로 클러스터링을 수행





### 병합 군집 (agglomerative)

- 계층적 클러스터링 알고리즘 중 하나
- 각 데이터 포인트를 개별적인 클러스터로 시작하여 서로 가장 비슷한 클러스터를 합치는 방식
- 데이터 포인트와 클러스터 간의 거리를 계산하여 클러스터를 합치며, 계층적인 클러스터 구조를 형성



### mean_shift

- 밀도 기반 군집화(density-based clustering) 알고리즘
- 주어진 데이터의 밀도를 기반으로 클러스터를 찾는 방법
- 데이터의 밀도가 가장 높은 지점을 중심으로 클러스터를 형성하며, 
- 데이터의 확률 밀도 함수를 추정하여 클러스터링을 수행



### dbscan

- 밀도 기반 군집화 알고리즘으로, 주어진 데이터의 밀도를 기반으로 클러스터를 찾는 방법
- DBSCAN은 데이터 포인트들을 중심으로 주어진 반경(epsilon) 내의 이웃 데이터 포인트들을 탐색하여 
- 밀도가 일정 이상인 코어 포인트(Core Point)들을 찾고, 이러한 코어 포인트들을 연결하여 클러스터를 형성



### 파이프라인 (Pipeline)

- 학습하기 위해 스케일링도 해야 하고 모델도 만들고 해야 하는데
- 파이프라인의 이름과 작업 객체를 여겨줬더니 알아서 fit 하고 
- 알아서 transform 하고 알아서 만들고
- 작업 여러개를 파이프라인으로 만들어두면 파이프 라인이 알아서 해줌



### 인코딩

- 원 핫 인코더 (OneHotEncoder)
  - 원 핫 인코더는 하나의 카테고리를 1로 표시하고 
  - 나머지 카테고리들을 0으로 표시하여 이진 형태로 변환
  - 변환된 데이터는 범주형 데이터의 고유한 값을 표현하기 때문에
  - 범주 간의 순서나 관계가 없는 경우에 유용하게 사용



- 엘보우 그래프 (elbow graph)
  - 클러스터링 알고리즘에서 최적의 클러스터 개수를 결정하는 데에 사용되는 그래프
  - 클러스터 개수를 x축으로, 적합도(또는 에러)를 y축으로 그리면 그래프의 형태가 팔꿈치(Elbow) 모양
  - 팔꿈치 모양이 나타나는 클러스터 개수가 최적의 클러스터 개수로 간주