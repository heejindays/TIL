**앙상블(ensemble) 이란?**

- 여러 개의 모델(classifier)을 사용해서 예측 결합함으로써 보다 정확한 최종 예측을 도출하는 방법
- 단일 모델보다 더 다양한 관점에서 데이터를 학습하고 예측하기 때문에 일반적으로 성능이 우수함
- 결과가 안 좋은 모델들(약한 분류기)을 모아서 좋은 결과를 내도록 하는 것



**앙상블의 종류**

- voting :  서로 다른 여러 모델의 예측을 조합하여 최종 예측을 수행
- hard voting (여러 모델 중 다수결로 더 많은 것 선택) / soft voting(가중치를 부여한 평균)
- Bagging : 같은 유형의 모델을 병렬 결합 (Bootstrapping aggregation) 
  - 같은 모델들이 데이터를 서로 다르게 학습
  - 배깅 안에 랜덤 포레스트(Random Forest)가 있음
  - Random Forest : dicision tree가 여러개 (bagging)

- Boosting : 여러 개의 모델들을 순차적으로 학습하여 
  - 결과에 대한 가중치를 부여 (이전모델의 학습 결과를 받아서 다시 사용)
  - 이전 모델이 잘못 예측한 샘플에 가중치를 부여하여 학습
  - (오답에 높은 가중치 > 오답을 정답으로 맞히기 위해)
  - Gradient Boosting : 이전 트리의 오차 보완 (boosting)

- Stacking : 위의 여러개의 개별적 모델을 학습하여 예측한 데이터셋을 최종 메타 모델이 사용하여 결과 도출

- Gradient Boosting : 이진 트리의 오차를 보완 > Boosting



**LightGBM (Light Gradient Boosting Machine)**

- Microsoft Reserch (MSR) 를 중심으로 한 팀에서 발표

- xgboost와 매우 비슷 (속도가 더 빠르고 가볍다)

- XGB 등 이전의 decision tree 기반 boosting은 높은 정밀도를 기대하는 반면

- 분할하는데 시간이 걸림



**LightGBM** **특징**

- level wise에서 leaf wise : 손실이 가장 줄어드는 노드에서 나누기

- histogram based : 연속값이 특징량을 히스토그램화 하여 bin 단위로 분할

- gradient-based one-side sampling (GOSS) : 기울기가 적은 데이터는 랜덤 샘플링

- Exclusive Feature Bundling (EFB) : 복수의 특징량을 bundle 하고 정리해 하나의 특징량과 같이 취급



**머신러닝 파트 용어 정리**

| 용어                                               | 내용                                                         |
| -------------------------------------------------- | ------------------------------------------------------------ |
| IQR  (Interquartile Range)                         | 상위 25%와 하위 25%  사이의 범위 (Q3 - Q1)     <br />데이터의 분포를 요약하고 이상치를 탐지하는 데 사용됨 |
| 표준화(Standardization)                            | 데이터에서  평균을 뺀 뒤 표준편차로 나누는 것     <br />데이터의 분포를 변환하여 평균을 중심으로 데이터가 어떻게 분포하는지 확인 |
| 표준 정규분포     (Standard Normal Distribution)   | 평균이  0이고 표준편차가 1인 정규분포     <br />정규분포를 표준화하여 얻은 분포로서, 평균을 중심으로 좌우 대칭이며 종 모양의 곡선을 가지는 확률 분포 |
| 도수분포표  (frequency distribution)               | 데이터의 값들이 특정 구간 또는 범주에  속하는 빈도(도수)를 보여주는 표 |
| ptp (peak to  peak)                                | 최대값과 최소값 사이의 차이                                  |
| 평균 제곱 오차      MSE(Mean Squared Error)        | 예측값과  실제값 간의 오차를 제곱한 후 평균     <br />값이 작을수록 모델의 예측이 실제값과 가깝다 |
| 회귀 분석 (Regression Analysis)                    | 종속  변수와 한 개 이상의 독립 변수간의 관계를 모델링하고 예측하기 위해 사용되는 통계적 분석 방법 |
| 선형 회귀(Linear  Regression)                      | 연속적인  데이터를 예측하고 싶을 때     <br />y는 출력 변수, x는 입력 변수, w는 가중치(회귀 계수)를 나타내며, b는 절편 |
| 사이킷 런  (scikit-learn)                          | 데이터 분석 및 예측 모델링을 위한  다양한 도구와 알고리즘을 제공하는 파이썬 머신러닝 라이브러리 |
| 결측치                                             | 데이터셋에서 값이 비어있거나 없는 경우                       |
| 지도학습  (Supervised Learning)                    | 입력 데이터와 이에 해당하는 정답  데이터을 사용하여 모델을 학습 |
| 비지도학습  (Unsupervised Learning)                | 데이터가 주어지지 않은 상황에서  패턴이나 구조를 발견하고 모델을 학습하는 방법 |
| 오버로딩  (Overloading)                            | 같은 이름을 가진 메서드나 함수를  다양한 매개변수로 정의하는 것 |
| 오버라이딩  (Overriding)                           | 상속 관계에서 자식 클래스가 부모  클래스의 메서드를 재정의하여 자신의 동작을 구현 |
| 머신러닝 (Machine  Learning)                       | 데이터를  통해 자동으로 학습하고 지식을 추출하며, 패턴을 인식하고 예측하는 인공지능 분야     <br />문제를 풀기 위해서 컴퓨터 시스템에 데이터를 학습시켜서 모델을 만드는 것      <br />지도학습, 비지도학습으로 구분 |
| 옵티마이저  (optimizer)                            | 옵티마이저는 모델의 가중치와 편향을  조정하여 손실 함수를 최소화하거나 성능 지표를 최대화하는 요소 |
| loss / cost  function                              | 머신러닝  모델의 성능을 평가하고 최적화하기 위해 사용되는 함수     <br />모델의 예측 값과 실제 값 사이의 오차를 계산하는 함수 |
| EDA(Exploratory  Data Analysis)                    | 데이터를  탐색하고 이해하기 위한 과정     <br />데이터셋의 구성, 변수 간의 관계, 이상치 및 결측치의 여부 등을 파악     <br />데이터의 특성을 시각화 및 요약하여 데이터의 패턴과 통계적 성질을 탐색 |
| metrix(평가지표)                                   | 모델의 성능을 측정하고 평가하는 데  사용되는 측정 항목 또는 지표 (정확도, 정밀도, 재현율 등) |
| MAE (Mean  Absolute Error)                         | 예측 값과 실제 값 간의 평균적인  절대적인 오차를 나타내는 지표 |
| MSE (Mean  Squared Error)                          | 예측 값과 실제 값 간의 평균적인 제곱  오차를 나타내는 지표   |
| RMSE (Root  Mean Squared Error)                    | 예측 값과 실제 값 간의 평균 제곱근  오차를 나타내는 지표     |
| R Squared  (결정계수)                              | 실제  종속 변수 값과 예측 값 간의 제곱 오차의 분산을 종속 변수 값의 분산으로 나눈 비율     <br />1에 가까울수록 모델이 데이터를 잘 설명, 0에 가까울수록 모델이 데이터를 잘 설명하지 못함 |
| 혼동 행렬  (confusion metrix)                      | 모델의  예측 결과를 실제 값과 비교하여 모델의 성능을 평가하는 데 사용되는 행렬     <br />예측된 클래스와 실제 클래스 간의 관계를 표현하며, 주로 이진 분류(Binary Classification)에서 사용 |
| ROC (Receiver  Operating Characteristic)           | 모델의  임계값(Threshold)을 변화시키면서,      <br />분류 모델의 민감도(Sensitivity)와 특이도(Specificity) 사이의 관계를 시각화     <br />x축에 1 - 특이도, y축에 민감도 |
| AUC (Area  Under the ROC Curve)                    | ROC  곡선의 아래 영역을 나타내는 지표     <br />0과 1 사이의 값을 가지며, 1에 가까울수록 모델의 성능이 좋다고 해석     <br />AUC가 0.5에 가까울 경우, 모델의 성능은 무작위 예측에 가까우며,      <br />0.5보다 낮을 경우 모델의 예측 성능이 뒤집혀 있음 |
| 스레드 홀드  (threshold)                           | 이진 분류(Binary  Classification) 모델에서 분류 결정을 내리는 기준값 |
| 하이퍼파라미터 튜닝  (Hyperparameter tuning)       | 머신  러닝 모델을 최적화하기 위해 하이퍼 파라미터들을 최적의 조합으로 찾는 과정     <br />모델의 학습 결과를 좀 더 올리기 위해서 스코어를 올리기 위해 파라미터를 바꿔보는 것 |
| 그리드 서치(Grid  Search)                          | 가능한 모든 조합을 대상으로  하이퍼파라미터 공간을 탐색하여 최적의 조합을 찾는 방법 중 하나 |
| cross_val_score                                    | 교차 검증(Cross  Validation)을 수행하여 모델의 성능을 평가하는 함수 |
| 검증  (Validation)                                 | 학습  데이터를 훈련에 사용하고, 모델의 매개변수를 조정하면서 훈련 데이터에 대한 성능을 측정     <br />모델의 성능을 평가하고 하이퍼파라미터 튜닝 등 모델의 설정을 조정하기 위해 사용되는 데이터 세트     <br />중간중간 모의고사를 보는 것처럼 중간중간 체크하려고 (나중에 학습할 때 넣어도 가능함) |
| 나이브 베이즈 (Naive  Bayes)                       | 확률  기반의 지도학습 알고리즘 중 하나로 각 특징(변수)이 서로 독립적이라고 가정     <br />간단하고 빠른 속도로 학습과 예측이 가능하며, 작은 크기의 훈련 데이터에도 잘 동작 |
| 의사결정나무  (Decision Tree)                      | 지도학습  알고리즘 중 하나로, 데이터의 속성과 레이블 간의 관계를 나타내는 분류 기법     <br />나무 구조를 가지며, 각 내부 노드는 속성을 테스트하고, 각 가지는 속성의 가능한 값에 대한 조건을 나타냄 |
| 불순도 (impurity)                                  | 의사결정나무  알고리즘에서 분할 기준을 결정하는 데 사용되는 개념     <br />불순도를 계산하는 데에 사용되는 지표로 Gini 불순도(Gini Impurity)와 엔트로피(Entropy)를 주로 사용 |
| KNN (k-Nearest  Neighbors)                         | 주어진  데이터 포인트 주변에 있는 k개의 가장 가까운 이웃 데이터 포인트들을 기반으로 예측을 수행     <br />간단하면서도 직관적인 알고리즘으로 새로운 데이터 포인트를 분류하거나 예측하기 위해 주변 이웃들의 레이블 또는 값에 기반하여 결정 |
| k겹 교차 검증 (k  fold cross validation)           | 주어진  데이터를 K개의 폴드(fold)로 나누고, 각 폴드를 한 번씩 검증 세트로 사용     <br />나머지 K-1개 폴드를 훈련 세트로 사용하여 모델을 학습하고 성능을 평가하는 과정을 K번 반복 |
| SVM (Support  Vector Machine)                      | 데이터  포인트들을 고차원의 특성 공간으로 매핑하여 결정 경계를 찾음     <br />클래스 간의 간격(마진)을 최대화하는 결정 경계를 찾는 것이 목표 |
| 스케일링(Scaling)                                  | 데이터의  범위를 조정하여 모든 특성이 동일한 스케일을 갖도록 하는 전처리 과정     <br />다양한 머신 러닝 알고리즘에서 중요한 단계로 고려되며,      <br />데이터의 크기나 범위가 다른 경우에도 모델의 성능을 향상 |
| 민맥스 스케일러  (MinMaxScaler)                    | 데이터의  값을 일정 범위로 스케일링하는 데 사용되는 전처리 기법 중 하나     <br />데이터의 최솟값과 최댓값을 이용하여 원래 데이터를 0과 1 사이의 범위로 변환 |
| StandardScaler                                     | 데이터의  평균과 표준편차를 이용하여 데이터를 표준화하는 데 사용되는 전처리 기법     <br />데이터를 평균이 0이고 표준편차가 1인 분포로 변환하여 데이터의 스케일을 조정 |
| Normalizer                                         | 데이터의  크기를 조정하기 위해 사용되는 전처리 기법     <br />각 데이터 포인트를 벡터의 길이로 나누어 데이터를 단위 길이로 조정 |
| 로지스틱 회귀 분석  (Logistic Regression Analysis) | 분류(Classification)  작업에 사용되는 지도학습 알고리즘으로 입력 값을 0과 1 사이의 확률 값으로 변환     입력 변수와 출력 변수(이진 레이블) 사이의 관계를 모델링 |
| 릿지 (Ridge)                                       | 과적합(Overfitting)을  줄이기 위해 사용되는 정규화 기법으로 L2 규제를 적용하여 모델의 가중치를 조정     L2 규제는 가중치들의 제곱의 합에 페널티를 부여하므로, 가중치의 크기를 작게 유지하고 일반화 성능을 향상 |
| 로쏘 (Lasso)                                       | 과적합(Overfitting)을  줄이기 위해 사용되는 정규화 기법으로 L1 규제를 적용하여 모델의 가중치를 조정     L1 규제 항은 가중치들의 절댓값의 합에 페널티를 부여하는 것 |
| 엘라스틱넷  (ElasticNet)                           | 선형 회귀의 한 종류로, L1 규제와  L2 규제를 함께 적용하는 정규화 기법 |
| 오버 피팅  (Overfitting)                           | 기계  학습 모델이 훈련 데이터에 너무 과도하게 적합되어, 새로운 입력 데이터에 대한 예측 능력이 저하되는 현상     <br />모델이 훈련 데이터의 노이즈까지 학습하여 일반화 능력을 잃게 될 때 발생 |
| 앙상블 (ensemble)                                  | 여러  개의 모델을 결합하여 더 강력하고 정확한 예측 모델을 만드는 기법     <br />각 모델은 독립적으로 학습하고 예측한 후, 그 결과를 조합하여 최종 예측 |
| 배깅(Bagging)                                      | 원본  데이터셋에서 중복을 허용하여 샘플을 추출하고 학습된 모델들의 예측 결과를 평균이나 다수결 등의 방식으로 결합하여 최종 예측을 수행 |
| 부트스트래핑  (Bootstrapping)                      | 원본 데이터셋에서 중복을 허용하여  샘플을 추출하는 통계적 리샘플링 방법 |
| 랜덤 포레스트  (Random Forest)                     | 배깅 앙상블의 대표적인 예로,  의사결정트리(Decision Tree)를 기반으로 한 모델 |
| 부스팅 (Boosting)                                  | 부스팅은  앙상블의 한 종류로, 약한 학습기(Weak Learner)를 여러 번 연속적으로 학습하고      <br />예측 오차를 보완하여 예측 모델을 구축 |
| learning_rate                                      | 각 모델의 기여 정도를 조절하는                               |
| max_depth                                          | 의사결정트리에서  최대 허용 깊이를 지정하는 매개변수     <br />트리의 깊이는 모델의 복잡성을 제어하며, 과적합을 방지하기 위해 적절한 값으로 설정 |
| softmax                                            | 분류 문제에서 다중 클래스 분류를  수행하기 위해 사용되는 활성화 함수 |
| softprop                                           | 신경망에서  사용되는 최적화 알고리즘 중 하나     <br />알고리즘과 유사한 방식으로 가중치를 조정하여 손실을 최소화 |
| XGBoost  (eXtreme Gradient Boosting)               | 그래디언트  부스팅 알고리즘을 기반으로 한 앙상블 학습 라이브러리     <br />트리 기반 모델의 성능을 향상시키고, 학습 속도와 예측 성능을 개선하는 다양한 기술을 도입 |
| 예측                                               | 모델이  학습된 후에 입력 데이터에 대해 결과를 추론하는 과정     <br />모델은 학습 데이터로부터 학습한 내용을 활용하여 새로운 데이터에 대한 예측 값을 생성 |
| 분류                                               | 주어진  입력 데이터를 사전에 정의된 클래스 레이블 중 하나에 할당하는 작업     <br />주로 범주형 데이터를 다루는데 사용 |
| train (학습)                                       | 모델이  주어진 학습 데이터에서 패턴과 특징을 학습하는 과정     <br />학습 데이터의 입력과 정답 레이블 사이의 관계를 파악하고, 최적의 매개변수를 찾아내는 방법을 학습 |
| test (모델 평가)                                   | 학습된  모델의 성능을 평가하기 위해 테스트 데이터를 사용하는 과정     <br />모델은 학습 데이터에서 학습한 내용을 바탕으로 테스트 데이터의 입력에 대한 예측을 수행하고,      <br />예측 결과를 정답 레이블과 비교하여 성능을 평가 |
| 군집분석                                           | 유사한  특성을 가진 데이터들을 그룹으로 묶는 비지도 학습 기법     <br />유사한 데이터끼리 같은 그룹에 속하고, 서로 다른 그룹 간에는 상이한 특성을 가지도록 데이터를 분할 |
| PCA (주성분 분석)                                  | 다차원  데이터의 특성을 압축하여 데이터의 주요한 구조를 파악하는 기법     <br />주성분 분석은 고차원 데이터의 분산을 최대화하는 방향으로 데이터를 변환하여 새로운 특성 축을 생성 |
| 그레디언트 부스팅  (Gradient Boosting)             | 이전  모델의 오차를 보완하는 방식으로 모델을 순차적으로 학습하는 알고리즘     <br />그래디언트(기울기)를 활용하여 모델을 보완 |
| LightGBM  (Light Gradient Boosting Machine)        | Microsoft에서 개발한 고성능 분산 그래디언트 부스팅 프레임워크     <br />xgboost와 매우 비슷 (속도가 더 빠르고 가볍다) |

