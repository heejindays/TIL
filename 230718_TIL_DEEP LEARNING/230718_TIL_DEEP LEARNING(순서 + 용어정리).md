### Tensorflow 분석 순서 (예 : 보스턴 집 값 데이터)



1. **데이터 준비 (x랑 y 값 준비)**

- x = pd.DataFrame(data, columns=["CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIDO", "B", "LSTAT"])
- y = pd.DataFrame(target, columns=["PRICE"])



2. **데이터 분할**

- x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)



3. **모델 준비**

   **3-1. Sequential()**

   model = Sequential()

   

   **3-2. layer 추가**

   **\# 데이터의 수에 비해 레이어가 너무 많으면 과적합**

   

   **\# input layer**

   model.add(Dense(32, activation="relu", input_shape=(x.columns.size, )))

   

   **\# hidden later** 

   \# (64개 : 원하는 수치를 쓰면 됨)

   \# (32개 : *** 이전 레이어의 아웃풋 레이어와 같음)

   model.add(Dense(64, activation="relu", input_shape=(32, )))

   

   \# (32개 : 원하는 수치를 쓰면 됨)

   \# (64개 : *** 이전 레이어의 아웃풋 레이어와 같음)

   model.add(Dense(32, activation="relu", input_shape=(64, )))

   

   **\# output layer**

   \# 1 : 출력될 개수가 하나라서(linear라서 y값이 하나여야 하니까)

   \# activation="linear" 그대로 linear로 y값 내보내야 하니까

   model.add(Dense(1, activation="linear"))

   

   

   **3-3. compile(optimizer="", loss="")**

   \# 모델의 나머지 추가 (옵티마이저 / 로스)

   model.compile(optimizer="adam", loss="mse")



4. **학습**

   result = model.fit(x_train, y_train, batch_size=10, epochs=100, validation_split=0.3)

   \# validation_split=0.3 : 30% 검증용으로 다시 사용할 것



5. **평가**

   model.evaluate(x_test, y_test)

   

6. **예측**

   predict = model.predict(x)



### 딥러닝 용어 정리 (7/18)

| 용어                               | 설명                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| Deep  Neural Network (DNN)         | 깊은 신경망, 여러 개의  은닉층(hidden layer)을 가진 신경망을 말하며, 보다 복잡한 모델을 구축할 수 있음  / 입력층(input layer), 은닉층(hidden layer), 출력층(output layer) 등으로 구성 / 다양한 특성을 학습하고 추상적인 표현을 학습할 수 있어, 이미지 처리, 자연어 처리 등 다양한 분야에서 좋은 성능 |
| MLP(Multi-Layer  Perceptron)       | 다중  은닉층을 가진 퍼셉트론 모델로, DNN의 한 종류  / 입력층, 여러 개의 은닉층, 출력층으로 구성되며, 각 층은 뉴런(노드)과 가중치로 연결  / 역전파(backpropagation) 알고리즘을 사용하여 가중치를 조정하며 학습을 진행 |
| 퍼셉트론(perceptron)               | 뉴런을 본따서 인공 신경을 만듦  / 여러 개 값이 입력되면 입력 데이터(x)와 가중치(weight)를 곱한 후 이를 다 합한 뒤 편향(bias)을 더하고,  그 결과를 활성화 함수(activation function)를 통과시켜 출력값을 계산     값을 여러 개 받은 뒤 합쳐서 함수를 통해서 결과를 출력함 |
| 대역폭  (bandwidth)                | 주로  클러스터링 알고리즘에서 사용되는 매개변수로, 데이터 포인트 간의 거리를 기준으로 클러스터의 너비를 결정 / Mean-Shift와 같은 알고리즘에서 사용되어 클러스터의 밀집도 추정에 활용 / 대역폭이 작으면 세밀한 구분을, 크면 더 넓은 범위를 가진 클러스터를 형성 |
| 활성화 함수  (Activation Function) | 퍼셉트론의  출력값을 결정하는 함수 / 활성화 함수는 입력의 가중치 합과 편향을 처리하여 최종 출력값을 생성 |
| 가중치 w(weight)                   | 인공 신경망에서 입력에 곱해지는 값으로, 각 입력의 중요도를 나타냄 |
| 절편, 편향b(bias)                  | 인공 신경망에서 입력에 더해지는 상수로, 가중치와 함께 선형 결합을 조정하는 역할 |
| 게이트(gate)                       | 디지털  논리 회로에서 사용되는 핵심 구성 요소  / 하나 이상의 입력 신호를 받아서, 이를 기반으로 출력 신호를 생성하는 역할 |
| AND 게이트                         | AND 게이트는 두 개 이상의 입력이  모두 참(1)일 때만 출력이 참(1)이 되고, 그렇지 않으면 출력은 거짓(0) |
| OR 게이트                          | OR 게이트는 두 개 이상의 입력 중  하나 이상이 참(1)이면 출력이 참(1), 모든 입력이 거짓(0)일 때 출력은 거짓(0) |
| NOT 게이트                         | NOT  게이트는 입력값을 반대로 뒤집어 출력 / 입력이 참(1)이면 출력은 거짓(0)이 되고, 입력이 거짓(0)이면 출력은 참(1) |
| Binary Step 함수                   | 간단한 활성화 함수 중 하나로,  입력값을 기준으로 이진 분류를 수행하는 함수 |
| 가우시안(Gaussian)  함수           | 정규  분포(Normal Distribution)를 표현하는 함수 / 데이터 분포에서 중심으로부터의 거리와 분포의 표준편차에 따라 값을 할당 |
| 시그모이드  함수(Sigmoid function) | 입력값을 확률값으로 변환하는 함수, 주로 이진 분류 문제에서 출력층의 활성화 함수로 사용됨 |
| 소프트맥스(Softmax)                | 다중  클래스 분류 문제에서 출력층의 활성화 함수로 사용 / 입력값을 각 클래스에 대한 확률값으로 변환, 각 클래스의 확률값의 합이 1이 되도록 정규화 |
| 렐루(ReLU, Rectified Linear  Unit) | 인공 신경망에서 가장  일반적으로 사용되는 활성화 함수 / 입력값이 0보다 큰 경우 그대로 출력하고, 0 이하인 경우 0으로 출력 |
| tensorflow                         | 구글에서  개발한 오픈소스 기계 학습 및 딥러닝 프레임워크 / 다양한 기능과 도구를 제공하여 기계 학습 모델을 구축하고 학습시키는 데 사용됨 |
| 레이어(Layer)                      | 인공  신경망에서 정보 처리 단위로 구성된 구성 요소 / 각 레이어는 입력 데이터를 받아서 연산을 수행하고 출력을 생성 / 입력 레이어, 은닉 레이어, 출력 레이어 등 다양한 유형의 레이어가 있으며,  각각의 레이어는 모델의 표현력을 증가시키고 복잡성을 조절하는 역할을 함 |
| SGD  (Stochastic Gradient Descent) | 최적화 알고리즘 중 하나로, 딥러닝에서  많이 사용되는 경사 하강법의 한 종류 |
| Dense                              | TensorFlow에서  제공하는 레이어 중 하나로 이전 레이어의 모든 노드와 연결된 형태 / 입력 데이터의 특성과 출력의 수를 지정하여 사용할 수 있으며, 활성화 함수를 적용할 수도 있음 |
| Epoch                              | 학습  과정에서 전체 훈련 데이터셋을 한 번 모델에게 보여주는 단위 / 각 Epoch마다 모델은 훈련 데이터셋을 사용하여 가중치를 업데이트하고 손실을 최소화하는 방향으로 학습을 진행 / 적절한 Epoch 수를 선택하는 것은 모델의 성능과 학습 시간에 영향을 미침 |
| val_loss                           | 검증  데이터셋에 대한 손실(loss) 값  / 해당 데이터셋에서 예측한 출력과 실제 값 사이의 차이  / 손실 값이 낮을수록 모델의 예측이 실제 값과 일치하거나 근사하다는 것을 의미  / 모델의 예측 값과 실제 값 간의 차이를 나타내는 지표로, 작을수록 모델의 예측이 실제 값과 일치한다는 의미 |
| 오차 역전파 (Back  Propagation)    | 신경망에서  가중치와 편향을 업데이트하기 위해 사용되는 알고리즘  / 신경망의 출력과 실제값 간의 오차를 이용하여 각 층의 가중치와 편향을 조정하는 방법 |