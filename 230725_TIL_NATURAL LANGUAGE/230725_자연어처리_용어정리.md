| 단어                               | 의미                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| 토큰  (token)                      | 문서를 작은 단위로 나누어  처리하는 과정에서의 최소 단위. 일반적으로 자연어 처리에서는 문장을  단어로, 문서를 문장으로 분리하여 각각의 토큰으로 취급 |
| 불용어  (stopwords)                | 불용어는 자연어 처리에서 분석에 큰  도움이 되지 않거나 중요도가 낮은 단어들. 일반적으로 자주 사용되는 관사, 전치사, 접속사 등이 불용어에 해당하며, 분석 시에 제거하여  처리 속도를 높이고 분석 결과의 정확성을 향상시킴. |
| 품사 태깅(pos  taging)             | 자연어 처리에서 각 단어의 문법적인  역할을 부여하는 과정. 문장 내 단어들에 명사, 동사, 형용사 등과 같은 품사를 태깅하여 문장의 구조를 파악하고 의미를 이해하는데 도움을  줌. |
| 렘마 (lemma)                       | 단어의 원형을 의미. 레마는 품사  정보와 결합하여 자연어 처리에서 단어들을 통일된 형태로 다루는데 활용 |
| 스템 (stem)                        | 스템은 단어의 접사를 제거하고 어근만을  남겨놓은 형태를 의미. 단어의 다양한 형태를 하나로 통일하여 처리하는데 사용 |
| vectorization                      | 자연어나 이미지 등의 데이터를 숫자  벡터로 변환하는 과정. 텍스트 데이터를 숫자로 변환하여 머신 러닝 알고리즘 등에 입력으로 활용할 수 있게 만드는데 사용. |
| n gram                             | 자연어 처리에서 연속된 n개의 단어들을  추출하는 방법. 2-gram은 문장에서 두 개의 연속된 단어를  추출하는 것이고, 3-gram은 세 개의 연속된 단어를 추출하는 것. 문맥을 파악하거나 언어 모델을 생성하는데 유용하게 사용. |
| bow(Bag-of-Words)                  | 자연어 처리에서 문서를 단어들의  가방으로 취급하는 방법. 문서 내에 단어들이 등장하는 빈도에만 집중하여 단어의 순서를 무시하고 벡터로 표현하는 방식. 각 단어는 고유한  인덱스를 가지며, 해당 인덱스 위치에 빈도수를 표시한 벡터로 문서를 표현 |
| 코사인 유사도                      | 두 벡터 사이의 유사도를 계산하는 방법  중 하나로, 벡터들 간의 각도를 이용하여 유사도를 측정. 두 벡터가 얼마나 같은 방향을 향하는지를 판단하여 유사도를 계산. 1에 가까울수록  유사하고, 0에 가까울수록 유사하지 않음. |
| tf-idf 유사도                      | 문서 간의 유사도를 측정하는 방법 중  하나로, TF-IDF 값들을 이용하여 계산. 각 문서에서의 단어 빈도와 역문서 빈도를 고려하여 단어들의 중요도를 반영한 벡터를 만들고,  이를 코사인 유사도를 사용하여 유사도를 측정 |
| 말뭉치(corpus)                     | 자연어 처리를 위해 수집된 텍스트  데이터의 집합을 의미. 웹문서, 뉴스 기사, 문학 작품 등의 다양한 텍스트 데이터가 말뭉치가 될 수 있음. |
| tanh  (Hyperbolic Tangent)         | 쌍곡선 탄젠트 함수로, 수학적인 함수  중 하나. 인공 신경망에서 활성화 함수로 자주 사용되며, 시그모이드 함수와 유사하게 사용됨. |
| LDA (Latent  Dirichlet Allocation) | 주어진 문서들에서 토픽들의 분포를  추론하여, 각 문서가 어떤 토픽들로 구성되어 있는지를 파악하는 작업을 수행. 비지도 학습 알고리즘으로, 텍스트 데이터에서 토픽을 추출하고  이를 기반으로 문서들을 그룹화하는 데 사용. |
| LSTM (Long  Short-Term Memory)     | 긴 시퀀스에 대한 정보를 유지하기 위한  기능을 갖추고 있음. 주로 시계열 데이터나 자연어 처리 등에서 활용 |
| GRU (Gated  Recurrent Unit)        | 장기 의존성 문제를 해결하기 위해  고안된 RNN의 변형 모델. LSTM에 비해 구조가 더 간단하며, 일부 연구에서는 성능이 유사하다고 보고되어 LSTM 대신 사용. 시퀀스  데이터 처리에 주로 활용 |
| NLTK (Natural  Language Toolkit)   | 자연어 처리를 위한 파이썬  라이브러리로, 텍스트 데이터를 처리하고 분석하는 데 사용. 문장 토큰화, 단어 토큰화, 품사 태깅 등 다양한 자연어 처리 작업을 수행 |
| RNN (Recurrent  Neural Network)    | 순환적인 구조를 가진 인공 신경망으로,  순서를 가진 시퀀스 데이터를 처리하는 데 적합. 각 단계마다 입력 데이터와 이전 단계의 출력 값을 함께 고려하여 작업을 수행. |
| 단어사전  (vocabulary)             | 자연어 처리에서 사용되는 단어들의  집합. 모든 단어들은 고유한 인덱스와 매핑되어 있으며, 이를 통해 텍스트 데이터를 숫자 벡터로 변환. 모델 학습 전에 텍스트 데이터를  단어사전을 기반으로 인덱싱하여 수치화하는 것이 일반적이며, 수치화된 데이터를 모델에 입력으로 사용하여 자연어 처리 작업을 수행. |